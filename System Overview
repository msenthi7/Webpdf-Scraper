1.outdir = './papers/'
unique=[]
for lnk in lnks:
    try:
        href=lnk['href']                              #This extracts the href attribute from each link lnk. The href is usually the part of a link that contains the actual URL                                                           (web address).

        if href.lower().endswith(('.pdf','.pptx')):   #The lower() method is used to convert the link to lowercase so that it doesn’t matter whether the link uses upper or                                                               lowercase letters.

            urld = ur.urljoin(url, href)               #This combines the base website address (url) with the file path from the link (href) to create the complete file URL.

                                                       #For example, if url is http://example.com/ and href is /files/document.pdf, ur.urljoin(url, href) would create the full                                                             URL http://example.com/files/document.pdf.
            rd = requests.get(urld, stream=True)
                                                        #This sends an HTTP GET request to the server to download the file from urld.
	                                                      The stream=True argument is used to download the file in chunks, which is useful for large files, so that the entire file                                                            is not loaded into memory at once.
                                                        
            filename = urld.split("/")[-1]              #Write the downloaded pdf to a file
                                                        #This extracts the actual filename from the URL. The split("/")[-1] breaks the URL string into parts wherever there is a /,                                                         and [-1] selects the last part, which is typically the filename.
            filehash = hashlib.md5(filename.encode('UTF-8')).hexdigest()

  The above line creates an MD5 hash (a kind of unique identifier) for the filename. The filename.encode('UTF-8') converts the filename into bytes, and hashlib.md5() generates       the hash.
	hexdigest() converts the hash into a readable string of hexadecimal characters.
	This is used to check if the file has already been downloaded based on its filename.
            if filehash not in unique: 
                    unique.append(filehash)
                    print(filename)
                    outfile = os.path.join(outdir,filename)

    #outdir is the directory where files are being saved (e.g., ./papers/), and os.path.join() combines this directory path with the filename to create the full file path.
	  #For example, if outdir is ./papers/ and filename is document.pdf, outfile will be ./papers/document.pdf.

                    with open(outfile,'wb') as f:
                        f.write(rd.content)
    #This opens a new file at the location outfile (e.g., ./papers/document.pdf) in write-binary mode ('wb').
	  #The with statement ensures the file is properly closed after writing, even if something goes wrong during the process.
    except:
        pass

    #The pass statement tells Python to skip over the error and move on to the next link without stopping the whole program.



NOTEE:
The os module in Python is used for interacting with the operating system, particularly for tasks related to file handling, directory management, and path manipulations. In the code you provided, the os module is used for the following purposes:

This Python script is used to download files (specifically PDFs and PPTX files) from a list of URLs (lnks). It checks each link, filters out the ones that end with .pdf or .pptx, and downloads them into a local directory (./papers/). It ensures each file is unique by using an MD5 hash of the filename and only downloading the file if its hash hasn’t been seen before.

1.	Imports:
	•	os: Used to handle file and directory operations.
	•	hashlib: Used to generate an MD5 hash of the filenames for checking uniqueness.
	•	urllib.parse: Used to construct valid URLs from relative or incomplete URLs.
2.	Directory Setup (outdir):
	•	The variable outdir is set to './papers/', meaning all downloaded files will be stored in the papers directory. If this directory doesn’t exist, you’d need to create it using     os.makedirs(outdir).
3.	Uniqueness Check (unique):
	•	unique is a list that stores the MD5 hashes of already downloaded filenames. It ensures that files with the same name are not downloaded multiple times.
4.	Loop Through Links (lnks):
	•	The lnks list is expected to contain links. For each link:
	            •	It extracts the href attribute, which is the URL of the file.
	            •	It checks if the link ends with .pdf or .pptx (case-insensitive).
	            •	If the condition is satisfied, it joins the base url and href to form the full URL using ur.urljoin.
5.	Downloading the File:
	•	requests.get(urld, stream=True) is used to send an HTTP GET request to download the file.
	•	The filename is extracted from the URL (urld.split("/")[-1]).
	•	An MD5 hash of the filename is generated using hashlib.md5.
6.	Check for Duplicate Files:
	•	The script checks if the file’s hash is already in the unique list. If not, it proceeds to download and save the file. The hash is then added to the unique list to ensure no      duplicate downloads.
7.	Saving the File:
	•	The file is saved in the outdir directory. The with open(outfile, 'wb') block writes the binary content of the file (rd.content) to the specified outfile.

8. BeautifulSoup: A popular Python library for parsing HTML and XML documents. It provides a simple way to navigate, search, and modify the parse tree.

example:
from bs4 import BeautifulSoup
html_content = "<html><body><p>Hello World!</p></body></html>"
soup = BeautifulSoup(html_content, 'html.parser')
print(soup.p.text)  # Output: Hello World!


Default parser html is assigned, otherwise.

Why Use an HTML Parser?

	•	Data Extraction: Extract information from web pages (e.g., scraping articles or product details).
	•	Data Cleaning: Remove or modify HTML tags and attributes for better data processing.
	•	Web Scraping: Automate the collection of data from web pages.

NOTE:
The <a> tag in HTML is used to create hyperlinks. It stands for “anchor” and is one of the most fundamental tags in HTML for linking web pages, files, or other resources. Here’s a detailed look at what the <a> tag is used for:

Similarly, 
The primary use of the <p> tag is to create paragraphs of text. Each <p> tag represents a separate paragraph, which is typically rendered with space above and below it to visually separate it from other elements.


9. response.content: This is the raw binary content of the HTTP response. It’s usually in bytes and contains the HTML content of the webpage. You would need to decode it to a string if you wanted to work with it as plain text. Thats why we converted it to limit the output screen to desired lines.


